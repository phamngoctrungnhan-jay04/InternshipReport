[
{
	"uri": "http://localhost:1313/workshop-template/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: ‚ÄúAWS Well-Architected Security Pillar‚Äù Event Objectives Deep dive into the 5 core pillars of AWS Security: IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response. Understand modern security principles: Zero Trust, Least Privilege, and Defense in Depth. Learn to automate security checks and incident response using AWS native tools. Identify common cloud threats in the Vietnam market and how to mitigate them. Key Highlights Opening \u0026amp; Security Foundation [Image of AWS Shared Responsibility Model] Core Principles: Moving beyond perimeter security to Defense in Depth, Zero Trust architecture, and strictly enforcing Least Privilege. Shared Responsibility Model: Clarifying what AWS secures (of the cloud) vs. what the customer secures (in the cloud). Local Context: Discussing top security threats specifically targeting cloud environments in Vietnam. Pillar 1: Identity \u0026amp; Access Management (IAM) [Image of IAM Identity Center architecture] Modern IAM Architecture: Moving away from long-term credentials (IAM Users) to temporary credentials (IAM Roles). Governance: Using IAM Identity Center for SSO and centralized management. Control: Implementing SCPs (Service Control Policies) and permission boundaries for multi-account environments. Best Practices: Enforcing MFA, regular credential rotation, and using Access Analyzer to validate policies. Pillar 2: Detection \u0026amp; Continuous Monitoring Logging Strategy: \u0026ldquo;Log everything\u0026rdquo; approach using CloudTrail (org-level), VPC Flow Logs, and ALB/S3 logs. Threat Detection: Utilizing Amazon GuardDuty for intelligent threat detection. Centralization: Aggregating findings in AWS Security Hub. Detection-as-Code: Automating alerts using Amazon EventBridge to trigger immediate notifications. Pillar 3: Infrastructure Protection [Image of AWS Network Security architecture] Network Security: Implementing rigorous VPC segmentation and distinguishing strictly between private and public subnets. Firewalls: Understanding the layered defense using WAF (Web Application Firewall), AWS Shield (DDoS), and Network Firewall. Access Control: Differentiating between Stateful (Security Groups) and Stateless (NACLs) firewalls. Pillar 4: Data Protection Encryption Strategy: At-rest: Encrypting S3 buckets, EBS volumes, RDS, and DynamoDB. In-transit: TLS/SSL enforcement. Key Management: Managing keys via AWS KMS (Key Management Service), focusing on grants and rotation policies. Secrets Management: Removing hardcoded credentials from code by using Secrets Manager and Systems Manager Parameter Store. Pillar 5: Incident Response (IR) [Image of Automated Incident Response workflow] IR Lifecycle: Preparation -\u0026gt; Detection \u0026amp; Analysis -\u0026gt; Containment, Eradication \u0026amp; Recovery -\u0026gt; Post-Incident Activity. Automation: Using AWS Lambda and Step Functions to auto-remediate issues (e.g., isolating a compromised EC2 instance). Playbooks: Walkthrough of standard responses for scenarios like \u0026ldquo;Compromised IAM Key,\u0026rdquo; \u0026ldquo;S3 Public Exposure,\u0026rdquo; and \u0026ldquo;Malware Detection.\u0026rdquo; Key Takeaways Identity is the New Perimeter Identity management is the most critical line of defense. Long-term access keys are a major risk; utilizing IAM Roles and SSO is mandatory for a modern architecture. Visibility is Paramount You cannot protect what you cannot see. Enabling centralized logging (CloudTrail, Config) and threat detection (GuardDuty) is the first step in any security strategy. Automate Security Humans are slow; attacks are fast. Security responses (locking down a user, blocking an IP) should be automated via code (Lambda/EventBridge) wherever possible. Applying to Work Audit IAM Policies: Review current permissions to ensure \u0026ldquo;Least Privilege\u0026rdquo; and remove unused IAM Users. Enable GuardDuty: Activate GuardDuty in the main region to detect anomalies immediately. Encrypt Data: Ensure all new S3 buckets and EBS volumes have default encryption enabled via KMS. Develop IR Playbooks: Draft a basic Incident Response plan for \u0026ldquo;S3 Public Leak\u0026rdquo; and \u0026ldquo;Compromised Credentials\u0026rdquo; scenarios. Event Experience The \u0026ldquo;AWS Well-Architected Security Pillar\u0026rdquo; workshop was an intensive and highly focused morning session. It provided a structured approach to security that is often overlooked in rushed development cycles.\nDeep Dive into \u0026ldquo;Defense in Depth\u0026rdquo; The session on Infrastructure Protection clarified how to layer security controls (WAF -\u0026gt; NACL -\u0026gt; SG) so that if one fails, others are still in place. Practical Focus on Automation Seeing the \u0026ldquo;Mini Demo\u0026rdquo; on validating IAM policies and simulating access was very helpful. It showed that security can be tested just like software code. The Incident Response segment changed my perspective: instead of waking up at 3 AM to fix a hack manually, we can write Lambda functions to contain threats automatically. Local Context Hearing about common pitfalls in Vietnamese enterprises helped me relate the theoretical concepts to the actual risks our business faces daily. Some event photos Add your event photos here\nOverall, this event reinforced that security is not just a \u0026ldquo;gatekeeper\u0026rdquo; but an enabler of speed when done correctly through automation and solid architecture.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: ‚ÄúDevOps on AWS‚Äù Event Objectives Understand the DevOps mindset, culture, and key performance metrics (DORA). Master the AWS CI/CD toolchain for automated software delivery. Learn Infrastructure as Code (IaC) principles using CloudFormation and CDK. Explore containerization strategies and observability best practices on AWS. Key Highlights DevOps Culture and Key Metrics Mindset Shift: Moving away from silos to shared responsibility between Development and Operations. Key Metrics (DORA): Focusing on Deployment Frequency, Lead Time for Changes, Mean Time to Restore (MTTR), and Change Failure Rate to measure success. Benefits: Faster innovation, higher reliability, and improved collaboration. AWS DevOps Services ‚Äì CI/CD Pipeline [Image of AWS CI/CD pipeline architecture] We explored the full automation pipeline, moving from manual deployments to orchestration:\nSource Control: Utilizing AWS CodeCommit and implementing Git strategies like GitFlow and Trunk-based development. Build \u0026amp; Test: Using AWS CodeBuild for compiling source code, running tests, and producing software packages. Deployment Strategies: implementing AWS CodeDeploy for Blue/Green, Canary, and Rolling updates to minimize downtime. Orchestration: Tying it all together with AWS CodePipeline. Infrastructure as Code (IaC) [Image of AWS CloudFormation workflow] Transitioning from manual console clicks to code-defined infrastructure:\nAWS CloudFormation: Using JSON/YAML templates to define resources, manage stacks, and detect infrastructure drift. AWS CDK (Cloud Development Kit): Defining cloud resources using familiar programming languages (Python, TypeScript, Java) to create reusable constructs. Tool Selection: Discussing when to use declarative templates (CloudFormation) vs. imperative code (CDK). Container Services on AWS [Image of Docker container architecture] Docker Fundamentals: Packaging applications into lightweight, portable containers. Registry: Using Amazon ECR for secure image storage and vulnerability scanning. Orchestration: Comparing Amazon ECS ( "
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "How BeyondTrust embedded Amazon QuickSight for identity security insights English (Original) üåê Read Original Blog Vietnamese (Translation) üáªüá≥ Read Vietnamese Translation "
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "How to securely deliver business intelligence to internal-facing applications with Amazon QuickSight English (Original) üåê Read Original Blog Vietnamese (Translation) üáªüá≥ Read Vietnamese Translation "
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Software developer career paths: 2025 job guide English (Original) üåê Read Original Blog Vietnamese (Translation) üáªüá≥ Read Vietnamese Translation "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-vpc/5.3.1-create-route-table/",
	"title": "Create Route table and Internet Gateway",
	"tags": [],
	"description": "",
	"content": " A Route Table is a set of rules (routes) used by a router to determine the best path for data packets to reach their destination.\nOpen the Amazon VPC console Choose Route tables, then click Create route table In the Create route table console: Specify name of Route table Choose VPC created Then click Create route table In Route table console, click Route table created Choose Route in navbar -\u0026gt; click Edit routes Edit routes console -\u0026gt; choose Target local -\u0026gt; Save changes Choose Internet Gateway in left navbar -\u0026gt; click Create internet gateway In Create Internet Gateway Specify name of Internet Gateway Click Create internet gateway Back to Route table -\u0026gt; Create route table like step 3 Click this Route table -\u0026gt; Edit routes In Target choose Internet Gateway created Then click **Save changes "
},
{
	"uri": "http://localhost:1313/workshop-template/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúGenAI-powered App-DB Modernization workshop‚Äù Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Ph·∫°m Ng·ªçc Trung Nh√¢n\nPhone Number: 0364369496\nEmail: nhanpntde180196@fpt.edu.vn\nUniversity: FPT University HCM Campus\nMajor: Information Technology\nClass: AWS\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 14/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "ApexEV ‚Äî Workshop Introduction (Short) ApexEV is an enterprise-grade EV garage management platform that digitizes workshop operations, improves customer experience, and helps technicians work faster and safer.\nWhy this workshop:\nSolve common garage problems: manual processes, poor transparency, weak customer care, and data risk. Build a secure, scalable and cost-efficient cloud backend from day one using AWS best practices. Core architecture (summary):\nFrontend: React app hosted on AWS Amplify (CI/CD + CloudFront). Backend: Spring Boot services in ECS (Fargate) ‚Äî serverless containers. Database: Amazon RDS (private subnets, automated backups, KMS encryption). Storage: Amazon S3 for media (use presigned URLs for direct uploads). API + Async: API Gateway as HTTPS ingress; SNS ‚Üí Lambda ‚Üí SES for email; Lambda ‚Üí Bedrock for AI/chat. Network \u0026amp; Security: VPC (public/private), Security Groups, VPC Endpoints, WAF and least-privilege IAM. Key benefits:\nSecurity-first: backend and DB remain private; edge services terminate TLS and enforce WAF/rate limits. Cost-aware: Fargate + Lambda (pay-per-use), lifecycle rules and autoscaling reduce costs. Modern \u0026amp; modular: frontend/backend separation, event-driven async flows for resilience and scale. Workshop goals:\nProvision network and secure services, deploy frontend + backend, connect RDS and S3, and integrate email and AI pipelines. Each module includes steps, recommended settings and cleanup instructions. "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Integrate into the AWS work culture and get to know new colleagues during the OJT process at First Cloud Journey (FCJ). Understand foundational knowledge of AWS Core Services. Practice operations on the AWS Management Console. Tasks carried out this week: Day Task Start Date Completion Date Reference Material Mon - Conducted preparatory research on AWS - Reviewed internship policies and working procedures at the FCJ unit 09/08/2025 09/08/2025 Tue - Studied Module 1: AWS infrastructure fundamentals and management tools + Overview of AWS global architecture + Management and cost-optimization principles Practice: Created and verified an AWS account 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=HxYZAK1coOI https://www.youtube.com/watch?v=IK59Zdd1poE https://www.youtube.com/watch?v=HSzrWGqo3ME https://www.youtube.com/watch?v=pjr5a-HYAjI https://www.youtube.com/watch?v=2PQYqH_HkXw https://www.youtube.com/watch?v=IY61YlmXQe8 https://www.youtube.com/watch?v=Hku7exDBURo Wed - Explored the AWS Management Console and the AWS CLI - Practice: + Hands-on navigation of the Console + Configured MFA and IAM basics + Installed and configured the AWS CLI 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ Thu - Studied Amazon EC2 fundamentals: + Instance types and sizing + Storage options: EBS vs. Instance Store + Network \u0026amp; security concepts: Security Groups, Key Pairs, Public/Private/Elastic IPs + IAM Roles and SSH access procedures 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ Fri - Practice: + Launched an EC2 instance and validated connectivity + Connected to the instance via SSH for verification 09/12/2025 09/12/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Became acquainted with the FCJ internship rules and working procedures.\nGained a clear overview of AWS global infrastructure, management tooling, and cost-optimization concepts covered in Module 1.\nCreated and verified an AWS Free Tier account.\nConfigured account security on the AWS Console:\nEnabled MFA (Multi-Factor Authentication). Created and managed basic IAM users and groups. Installed and configured the AWS CLI on a local workstation.\nConsolidated core Amazon EC2 knowledge, including:\nInstance types and sizing considerations. Storage alternatives (EBS vs. Instance Store). Network and security primitives: Security Groups, Key Pairs, and IP addressing distinctions. The purpose and usage of IAM Roles for delegated permissions. Performed hands-on EC2 exercises:\nLaunched an EC2 instance and verified operation. Connected to the instance using SSH and executed basic interactions. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Deeply understand AWS networking architecture: VPC, security features, and Multi-VPC models. Master advanced networking concepts: VPN, DirectConnect, and LoadBalancer. Practice building a complete network system: Subnet, Route Table, Internet Gateway, Security Group, and Network ACLs. Connect and establish teamwork with final project team members. Tasks executed this week: Day Task Start Date End Date Resources Mon - Reviewed AWS Virtual Private Cloud (VPC) concepts - Examined VPC security capabilities and multi-VPC design patterns 15/09/2025 15/09/2025 https://www.youtube.com/watch?v=O9Ac_vGHquM https://www.youtube.com/watch?v=BPuD1l2hEQ4\u0026t Tue - Studied VPN, AWS Direct Connect, and Load Balancing mechanisms; collected supplementary resources 16/09/2025 16/09/2025 https://www.youtube.com/watch?v=CXU8D3kyxIc\u0026t Wed - Practice: + Explored VPC configuration + Created subnets + Configured route tables and Internet Gateway 17/09/2025 17/09/2025 https://www.youtube.com/watch?v=dHoYmQR7FYs https://www.youtube.com/watch?v=XBJgHS3XQjk Thu - Practiced configuring Security Groups and Network ACLs - Explored the VPC Resource Map utility for visualizing resources 18/09/2025 18/09/2025 https://www.youtube.com/watch?v=B1qxOQLmavQ https://www.youtube.com/watch?v=GVDsDu9dOFY\u0026t https://www.youtube.com/watch?v=fZa_kQ69stI Fri - Reviewed weekly findings and deepened VPC understanding - Attended initial meeting with the final project team to align goals 19/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Developed a clear understanding of key Amazon VPC elements:\nCIDR notation and subnetting Public vs. Private subnets Route tables and Internet Gateways (IGW) Acquired knowledge of connectivity and load distribution options:\nVPN and AWS Direct Connect for hybrid connectivity Load balancer fundamentals for traffic distribution Implemented network security constructs:\nConfigured Security Groups (instance-level, stateful) Configured Network ACLs (subnet-level, stateless) Utilized the VPC Resource Map to visualize network topology and data flow.\nHeld the kickoff meeting with the project team to coordinate next steps.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Master EC2 compute services: Lifecycle, Storage (EBS/Instance Store), and Auto Scaling. Implement advanced connectivity using AWS Transit Gateway. Deploy scalable storage and content delivery solutions (S3 \u0026amp; CloudFront). Explore foundational AI/ML concepts (NLP, Sentiment Analysis) for the final project. Collaborate with the team to brainstorm and finalize the final project idea. Tasks completed this week: Day Task Start Date End Date Resources Mon - Provisioned EC2 instances within subnets - Practiced creating Internet Gateways - Reviewed Transit Gateway route table concepts and connectivity - Verified EC2-to-endpoint connectivity 22/09/2025 22/09/2025 https://www.youtube.com/@AWSStudyGroup/ https://cloudjourney.awsstudygroup.com/ Tue - Conducted an in-depth study of EC2 features: AMIs, backups, key pairs, EBS, instance store, user-data/metadata, and Auto Scaling 23/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/@AWSStudyGroup/ Wed - Deployed infrastructure components - Established a backup plan and executed recovery tests - Cleaned up temporary resources - Created an S3 bucket for storage 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/@AWSStudyGroup/ Thu - Provisioned EC2 for Storage Gateway tasks - Deployed a simple static website - Configured S3 public access settings and object permissions - Experimented with AWS CloudFront configuration to accelerate content delivery 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/@AWSStudyGroup/ Fri - Introduced supervised ML techniques and sentiment analysis - Performed basic NLP preprocessing - Visualized tweet datasets and experimented with logistic regression models - Held a team brainstorming session to refine the final project concept 26/09/2025 26/09/2025 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/ Week 3 Achievements: Managed and optimized EC2 instances, including AMI handling, key-pair setup, user-data, and storage configuration.\nDistinguished storage options (EBS vs. Instance Store) and implemented Auto Scaling for resilience.\nDesigned and validated inter-VPC connectivity using AWS Transit Gateway.\nImplemented storage and recovery procedures:\nEstablished backup plans and executed recovery tests. Deployed a static website on S3 and configured object access controls. Improved content delivery by integrating AWS CloudFront with S3.\nInitiated work on Machine Learning concepts (NLP):\nCovered supervised methods and sentiment analysis fundamentals. Conducted basic NLP preprocessing and exploratory model visualization. Formulated an initial concept for the final project. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Master Hybrid Cloud workflows: Importing/Exporting Virtual Machines (VMs) and integrating On-premises storage. Deepen knowledge of AWS Storage (EFS, FSx, Storage Gateway) and Compute (Autoscaling, Lightsail). Build the mathematical foundation for Natural Language Processing (NLP): Vector Spaces, Probability, and Linear Algebra. Finalize the semester project concept and setup the documentation framework using Hugo. Tasks executed this week: Day Task Start Date End Date Resources Mon - Studied EC2 Auto Scaling, EFS/FSx, and Lightsail - Reinforced core storage and compute concepts - Reviewed probability basics and Bayes‚Äô theorem 29/09/2025 29/09/2025 https://www.youtube.com/@AWSStudyGroup/ https://www.coursera.org/learn/classification-vector-spaces-in-nlp/ Tue - Explored AWS database and security services - Practiced VM export/import workflows using VMware Workstation and AWS import tools 30/09/2025 30/09/2025 https://www.youtube.com/@AWSStudyGroup/ Wed - Practiced linear algebra in Python using NumPy - Implemented Euclidean distance and cosine similarity calculations - Explored word vector manipulation and Vector Space Model exercises 01/10/2025 01/10/2025 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/ Thu - Imported virtual machines into AWS and instantiated instances from AMIs - Configured S3 ACLs and exported VM images as needed - Deployed an AWS Storage Gateway and mounted file shares on on-premises systems - Cleaned up temporary cloud resources 02/10/2025 02/10/2025 https://www.youtube.com/@AWSStudyGroup/ Fri - Reviewed Hugo themes and documentation best practices - Outlined the final workshop report structure - Held a wrap-up meeting to finalize the project idea and documentation plan 03/10/2025 03/10/2025 https://www.youtube.com/@AWSStudyGroup/ Week 4 Achievements: Hybrid cloud procedures validated:\nExported VMs from VMware Workstation and uploaded images to AWS. Created AMIs and launched instances from those images. Demonstrated the ability to move instances between cloud and local environments. Advanced storage solutions configured:\nDeployed and tested an AWS Storage Gateway. Mounted cloud file shares on on-premises hosts. Managed S3 ACLs to ensure appropriate access controls. Mathematical groundwork for NLP established:\nUsed Python/NumPy for linear algebra computations. Implemented Euclidean distance and cosine similarity for text vectors. Reviewed Bayes‚Äô theorem and Vector Space Models. Documentation \u0026amp; project planning:\nChose and configured a Hugo theme for reporting. Finalized the project idea and documentation outline with the team. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Master advanced NLP algorithms: Vector transformation, K-nearest neighbors (KNN), and Hash tables. Deep dive into the AWS Storage ecosystem: S3 advanced features, Glacier, Snow Family, and Backup strategies. Implement Security compliance and Automation: Security Hub, IAM Roles, and AWS Lambda. Finalize the detailed execution plan for the final semester project. Tasks completed this week: Day Task Start Date End Date Resources Mon - Studied word vector transformations - Implemented K-nearest neighbors (KNN) concepts - Reviewed hash tables and hashing techniques - Completed the Word Translation code lab 06/10/2025 06/10/2025 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/ Tue - Explored AWS storage services including S3 access points, storage classes, static website hosting and CORS - Reviewed Glacier archival options, AWS Snow Family, Storage Gateway, and backup strategies 07/10/2025 07/10/2025 https://www.youtube.com/@AWSStudyGroup/ Wed - Executed Lab Module 05: + Enabled Security Hub and assessed compliance + Provisioned VPC, Security Group and EC2 instance + Tagged resources and created an IAM role for Lambda + Implemented a Lambda function, verified behavior, and cleaned up resources 08/10/2025 08/10/2025 https://www.youtube.com/@AWSStudyGroup/ Thu - Translated documentation content as needed - Continued lab exercises (Module 05-28 to 05-30) - Held planning meetings to refine the final project schedule 09/10/2025 09/10/2025 https://www.youtube.com/@AWSStudyGroup/ Fri - Completed lab modules 05-31 through 05-33 - Convened a team meeting to consolidate the final project plan - Conducted a weekly review to reinforce learned concepts 10/10/2025 10/10/2025 https://www.youtube.com/@AWSStudyGroup/ Week 5 Achievements: Key accomplishments: Advanced NLP techniques and vector arithmetic:\nImplemented KNN and explored hash-table based methods. Completed a Word Translation lab leveraging vector transformations. Advanced AWS storage capabilities:\nDeployed S3 static hosting with CORS and managed storage classes. Reviewed Glacier archival workflows and Snow Family migration options. Security and automation:\nEnabled AWS Security Hub and evaluated compliance scores. Built serverless automation with AWS Lambda and appropriate IAM roles. Applied tagging best practices for EC2 resource management. Project milestones:\nProgressed through the Module 05 lab series. Finalized the detailed execution roadmap for the final project. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Master AWS Security fundamentals: Shared Responsibility Model, Identity Center, and Key Management. Develop skills in designing and visualizing Cloud Architecture using professional standards (Draw.io). Deep dive into NLP Attention Models: Seq2seq, Neural Machine Translation (NMT), and Evaluation metrics. Collaborate with the team to draft and refine the High-Level Architecture for the final project. Tasks executed this week: Day Task Start Date End Date Resources Mon - Practiced creating AWS architecture diagrams using Draw.io with AWS icon sets - Completed Lab modules 05-44 and 05-48 13/10/2025 13/10/2025 https://www.youtube.com/@AWSStudyGroup/ Tue - Studied the Shared Responsibility Model and AWS identity services: IAM, Cognito, Organizations, and Identity Center - Reviewed KMS and Security Hub; supplemented learning with hands-on practice 14/10/2025 14/10/2025 https://www.youtube.com/@AWSStudyGroup/ Wed - Conducted a design meeting to produce a high-level architecture diagram - Studied Seq2Seq architectures and attention mechanisms (queries, keys, values) - Reviewed NMT concepts and evaluation metrics (BLEU, ROUGE-N) and decoding techniques such as Beam Search and Minimum Bayes Risk 15/10/2025 15/10/2025 https://www.coursera.org/learn/attention-models-in-nlp/ Thu - Refined AWS architecture diagramming skills and iteratively edited diagrams based on feedback 16/10/2025 16/10/2025 https://www.youtube.com/@AWSStudyGroup/ Fri - Reviewed and consolidated weekly knowledge - Iterated on the architecture diagram incorporating mentor feedback - Held a team discussion to agree on the diagram\u0026rsquo;s final structure 17/10/2025 17/10/2025 https://www.youtube.com/@AWSStudyGroup/ Week 6 Achievements: Security and identity:\nClarified the Shared Responsibility Model. Configured IAM and AWS Identity Center for administrative tasks. Investigated Cognito for application authentication and evaluated KMS for key management. Architecture design:\nBecame proficient with Draw.io and AWS iconography. Drafted and iterated on the project\u0026rsquo;s high-level architecture diagram. Incorporated mentor feedback into subsequent revisions. Advanced NLP topics:\nStudied Seq2Seq and attention mechanisms, focusing on queries, keys, and values. Reviewed model evaluation metrics (BLEU, ROUGE-N) and decoding strategies like Beam Search. Team coordination:\nAligned team members on architectural direction through focused design meetings. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Master AWS Database services: RDS, Aurora, Redshift, and ElastiCache. Develop practical AI skills: Research and implement an AI Chatbot. Finalize the High-Level Architecture Diagram based on mentor feedback. Kick-start the Final Project implementation: Frontend and Backend design. Tasks completed this week: Day Task Start Date End Date Resources Mon - Completed Lab Module 06 and further iterated on the architecture diagram 20/10/2025 20/10/2025 https://www.youtube.com/@AWSStudyGroup/ Tue - Studied database concepts and AWS database offerings (RDS, Aurora, Redshift) - Reviewed caching with ElastiCache - Researched AI chatbot development approaches 21/10/2025 21/10/2025 https://www.youtube.com/@AWSStudyGroup/ Wed - Prototyped an AI chatbot and supplemented research on chatbot architectures - Incorporated mentor feedback into the system diagram 22/10/2025 22/10/2025 https://www.youtube.com/ Thu - Initiated the final project implementation and drafted front-end and back-end designs 23/10/2025 23/10/2025 Fri - Consolidated weekly learnings - Continued diagram revisions with peer feedback - Tested AI prototype and advanced the front-end/back-end designs 24/10/2025 24/10/2025 Week 7 Achievements: [Image of AWS RDS architecture]\nDatabase expertise:\nClarified distinctions between relational services (RDS, Aurora) and data warehousing (Redshift). Evaluated caching strategies using Amazon ElastiCache. AI prototyping:\nResearched and prototyped an AI chatbot integrated into the project scope. Architecture maturity:\nFinalized the system architecture after iterative reviews. Project kick-off:\nBegan development for the final project with initial front-end and back-end designs. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Finalize the System Architecture and integrate the AI module into the main project. Complete the core development phase: Basic Frontend, Backend, and Project Proposal. Comprehensive review of all AWS modules and supplementary knowledge covered to date. Successfully complete the Midterm Exam. Tasks carried out this week: Day Task Start Date End Date Resources Mon - Revised the architecture diagram and refined service choices - Finalized the AI component and began integration with the application 27/10/2025 27/10/2025 Tue - Drafted the Project Proposal - Held team collaboration sessions - Implemented the basic front-end and back-end components 28/10/2025 28/10/2025 Wed - Conducted a thorough review of accumulated knowledge and supplemental materials 29/10/2025 29/10/2025 https://www.youtube.com/@AWSStudyGroup/ https://cloudjourney.awsstudygroup.com/ Thu - Continued knowledge consolidation and review of supplementary resources 30/10/2025 30/10/2025 https://www.youtube.com/@AWSStudyGroup/ https://cloudjourney.awsstudygroup.com/ Fri - Took the Midterm Exam - Held a meeting to adjust and prioritize project features 31/10/2025 31/10/2025 Week 8 Achievements: [Image of full stack development workflow]\nSystem integration and optimization:\nIntegrated the AI chatbot into the main application stack. Refined the AWS architecture to better match implementation constraints. Development progress:\nImplemented core front-end and back-end components. Completed and documented the Project Proposal. Knowledge consolidation and evaluation:\nReviewed core AWS modules and supplementary materials. Successfully completed the Midterm Exam. Iterative improvements:\nPrioritized and adjusted features following team discussions and test feedback. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Deep dive into modern NLP architectures: Transformers, Attention Mechanisms, and Decoders. Explore Transfer Learning and Large Language Models (LLMs) like BERT, GPT, and T5. Implement Generative AI solutions using AWS Bedrock. Optimize the Final Project architecture and refine the Chatbot\u0026rsquo;s performance/UI. Tasks performed this week: Day Task Start Date End Date Resources Mon - Studied differences between Transformers and RNNs - Reviewed scaled dot-product attention and masked self-attention - Analyzed transformer decoders 03/11/2025 03/11/2025 https://www.coursera.org/learn/attention-models-in-nlp/ Tue - Completed a Transformer summarization lab - Optimized the chatbot logic and refined the user interface 04/11/2025 04/11/2025 https://www.coursera.org/learn/attention-models-in-nlp/ Wed - Held a meeting to optimize the AWS service selection - Studied transfer learning techniques and surveyed major LLMs (ELMo, GPT, BERT, T5) 05/11/2025 05/11/2025 https://www.coursera.org/learn/attention-models-in-nlp/ Thu - Explored AWS Bedrock and experimented with integrating it into the AI chatbot 06/11/2025 06/11/2025 Fri - Prepared and curated training/response data for the Bedrock-backed chatbot - Performed testing and resolved data-processing issues 07/11/2025 07/11/2025 Week 9 Achievements: Advanced NLP architectures:\nDistinguished Transformers from RNN architectures and examined attention variants. Implemented a Transformer-based summarizer in practical labs. Large language model exposure:\nStudied transfer learning paradigms and surveyed prominent LLMs (ELMo, GPT, BERT, T5). Generative AI on AWS:\nIntegrated AWS Bedrock features to enhance the chatbot\u0026rsquo;s generative capabilities. Curated and preprocessed context data and debugged integration issues. Product refinement:\nImproved chatbot UI and optimized the chosen AWS service stack for the final deployment. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "The internship roadmap was divided into three main phases:\nFoundational Knowledge: Mastering AWS core services (Networking, Compute, Storage, Security) and Math for ML. Advanced Research: Deep diving into NLP, Transformers, and Generative AI (AWS Bedrock). Project Implementation: System architecture design, Full-stack development, and Serverless deployment. Below is the weekly breakdown of tasks and achievements:\nWeek 1: Introduction to Cloud Computing \u0026amp; AWS Account Setup\nWeek 2: Deep Dive into AWS Networking: VPC \u0026amp; Security\nWeek 3: Compute, Connectivity (Transit Gateway) \u0026amp; Intro to NLP\nWeek 4: Hybrid Cloud Solutions, Storage Gateway \u0026amp; Math for NLP\nWeek 5: Advanced Storage, Security Automation \u0026amp; NLP Algorithms\nWeek 6: Cloud Architecture Design, Identity Security \u0026amp; Attention Models\nWeek 7: Databases, AI Chatbot Prototyping \u0026amp; Project Kick-off\nWeek 8: System Integration, Midterm Exam \u0026amp; Project Proposal\nWeek 9: Transformers, LLMs \u0026amp; Generative AI with AWS Bedrock\nWeek 10: BERT Fine-tuning, Serverless Optimization \u0026amp; UI Completion\nWeek 11: Documentation, Final Deployment \u0026amp; Architectural Synchronization\nWeek 12: Final Review, Bug Fixing \u0026amp; Product Demo\nThis section provides a concise weekly log of activities, objectives, and accomplishments throughout the internship. Each week contains a list of planned tasks, references used, and the key outcomes achieved.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-vpc/5.3.3-create-security-groups/",
	"title": "Create Security Groups",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console Choose Security Groups -\u0026gt; click Create security group In Create Security group Spacific name of Security group Choose VPC created Add rule Inbound and Outbound for Security Group In ReGenZet project we have 4 Security groups, which are fargate-sg, rds-sg, alb-sg and endpoint-sg. fargate-sg is security group for AWS ECS Fargate Do again step 1 -\u0026gt; 3 Choose fargate-sg created Inbount Add rule security group of Application Load Balancer (ALB) is alb-sg Outbound Add rule security group of MySQl (rds-sg) and HTTPS Click Create security group rds-sg is security group for AWS RDS Do again step 1 -\u0026gt; 3 Choose rds-sg created Inbound Add rule security group of MySQL like this instruct Outbound is not Add rule Click Create security group alb-sg is security group for AWS Application Load Balancer (ALB) Do again step 1 -\u0026gt; 3 Inbound Add rule HTTPS and HTTP type like this instruct Outbound Add rule security group of AWS ECS Fargate (fargate-sg) Click Create security group endpoint-sg is security group for VPC Endpoints Do again step 1 -\u0026gt; 3 Inbound Add rule security group of AWS ECS Fargate (fargate-sg) Outbound is not Add rule "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-vpc/5.3.2-create-subnets/",
	"title": "Create Subnets",
	"tags": [],
	"description": "",
	"content": "Create Public Subnet Open the Amazon VPC console Choose Subnets -\u0026gt; click Create subnet In Create subnet console: Choose VPC created Fill subnet name Choose AZ Spacific IPv4 subnet Then click Create subnet Create Private Subnet Do again step 1 -\u0026gt; 4 Click this Subnet -\u0026gt; Choose Route table Choose Route table ID is private Click Save "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-vpc/5.3.4-create-vpc-endpoints/",
	"title": "Create VPC Endpoints",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console Choose Endpoints -\u0026gt; click Create endpoints In Create console: Fill name of VPC endpoint Type is AWS Services Search In this project we have 5 VPC Endpoints VPC Enpoint S3 Gateway (apexev-s3-gateway) In search box -\u0026gt; com.amazonaws.ap-southeast-1.s3 Choose type Gateway Choose VPC created Choose Route table private Policy is Full access Click Create endpoint VPC Enpoint ECR API \u0026amp; DKR Interface In search box -\u0026gt; ecr Will see ecr.api and ecr.dkr interface Do this twice and choose a different one each time Choose VPC created Tick two subnet private in two difference AZ Choose endpoint-sg Click Create endpoint VPC Enpoint Logs In search box -\u0026gt; com.amazonaws.ap-southeast-1.logs Choose com.amazonaws.ap-southeast-1.logs Choose VPC created Tick two subnet private in two difference AZ Choose endpoint-sg Click Create endpoint VPC Enpoint Logs In search box -\u0026gt; com.amazonaws.ap-southeast-1.sns Choose com.amazonaws.ap-southeast-1.sns Choose VPC created Tick two subnet private in two difference AZ Choose endpoint-sg Click Create endpoint "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.2-project-architecture/",
	"title": "Project Architecture",
	"tags": [],
	"description": "",
	"content": "Architecture Overview This document describes the recommended production-ready architecture for ReGenZet ‚Äî an EV Garage Management System ‚Äî and explains the technology choices used in the workshop.\nFrontend: A React single-page application deployed and hosted via AWS Amplify. Amplify provides automated CI/CD, asset hosting and integration with CloudFront for global caching and fast client delivery. Backend: Containerized Spring Boot services packaged as Docker images and deployed to Amazon ECS (Fargate). Fargate removes host management overhead and provides scalable, serverless compute for microservices. Database: Amazon RDS (MySQL/Postgres) hosted in private subnets. RDS provides automated backups, snapshots, Multi-AZ options and encryption at rest (KMS). API Management: Amazon API Gateway exposes a single HTTPS entry point for client traffic, handles request routing, TLS termination and request throttling. Media Storage: Amazon S3 holds vehicle inspection images, videos and other media. Use presigned URLs for secure direct-upload/download to offload traffic from the backend. Asynchronous / Serverless components: Email pipeline: Backend (Spring) publishes an event to SNS, which triggers a Lambda to send email via Amazon SES. AI/Chat pipeline: Frontend ‚Üí API Gateway ‚Üí Lambda ‚Üí Amazon Bedrock (or other managed LLM) for inference and conversational workflows. Network \u0026amp; Security: Deploy resources inside a VPC with well-defined Public and Private subnets. Use Security Groups and NACLs to control traffic. Use VPC Endpoints (Gateway and Interface) for S3 and private service access, keeping traffic inside the AWS network. Why this architecture? Security-first\nThe backend and RDS instances reside in private subnets and never expose database ports to the public internet. API Gateway and load-balanced frontends terminate TLS at the edge, while internal services communicate over private networking. Cost-efficiency\nFargate and Lambda offer a pay-for-what-you-use model. When appropriate, consider Fargate Spot for non-critical workloads and configure autoscaling and lifecycle policies for S3 to reduce costs. Operational simplicity \u0026amp; modern patterns\nClear separation of concerns between frontend and backend, with API Gateway as a single ingress point. Event-driven components (SNS, Lambda) decouple email and AI processing from request-response paths, improving resilience and scalability. Developer productivity\nAWS Amplify simplifies frontend CI/CD and hosting. Container workflows with Docker + ECR and ECS Fargate enable reproducible deployments for backend services. Security and best-practice highlights\nLeast-privilege IAM roles for services and cross-account access where needed. KMS-managed encryption for RDS and S3 objects. WAF and rate-limiting on API Gateway to mitigate application-level attacks. This architecture balances enterprise-grade security with cost-effective serverless patterns and provides a pragmatic path for incremental adoption of advanced features (observability, multi-region DR, Bedrock-powered AI).\n"
},
{
	"uri": "http://localhost:1313/workshop-template/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "APEX-EV Electric Vehicle Service Platform 1. Executive Summary RenGen is a comprehensive management platform designed to digitize and optimize maintenance workflows at service centers. The system centrally manages the entire service lifecycle‚Äîfrom request intake and repair processing to customer care‚Äîhelping to eliminate manual tasks and enhance efficiency. Leveraging the power of the AWS cloud, RenGen combines flexible container architecture on Amazon ECS Fargate with the intelligent processing capabilities of Generative AI through Amazon Bedrock. The solution integrates automated development processes (CI/CD) from GitLab, ensuring rapid deployment speeds, high security, and rigorous monitoring, delivering a superior experience for end-users.\n2. Problem Statement What‚Äôs the Problem? Current operational processes rely heavily on manual methods, leading to inefficiencies, fragmented data, and a lack of intelligent support tools for automated customer interaction.\nThe Solution The platform employs a modern architecture, starting at the Edge layer with Amazon Route 53 for user routing. The interface (Frontend) is hosted on AWS Amplify Hosting, ensuring fast and stable access. Amazon API Gateway acts as the central hub, intelligently routing requests.\nTo ensure security, critical components such as ECS Fargate and the Amazon RDS database are placed in a Private Subnet, completely isolated from the public internet. Image data is stored on Amazon S3 and accessed securely via S3 Endpoints. Additionally, the software development process is fully automated: source code from GitLab is packaged and pushed to Amazon ECR for deployment to ECS.\nBenefits and Return on Investment Adopting this architecture delivers a significant competitive advantage by integrating Artificial Intelligence (GenAI) via Amazon Bedrock, which helps automate customer care and data analysis. The system ensures high availability and data security thanks to the VPC network separation design (Public/Private subnets).\nThe CI/CD process integrated with GitLab and ECR helps minimize downtime when updating new features, while Amazon CloudWatch provides comprehensive monitoring to detect incidents instantly. The cost model is optimized thanks to the use of Fargate (Serverless container) and Lambda (Pay-per-use), ensuring businesses only pay for the actual resources used. This investment not only resolves current operational challenges but also creates a solid technological foundation for long-term growth, with the expected Return on Investment (ROI) period significantly shortened.\n3. Solution Architecture The RenGen management platform utilizes a modern architecture deployed on AWS (Region ap-southeast-2), initiated by user access via Amazon Route 53 at the Edge layer. The User Interface (Frontend) is hosted on AWS Amplify Hosting, which establishes a direct connection to Amazon API Gateway as the central entry point.\nFrom the API Gateway, the data flow is strategically divided into three distinct paths:\nAI Tasks: Requests are routed to AWS Lambda to interact with Amazon Bedrock for generative AI capabilities. Notification Tasks: Asynchronous requests trigger AWS Lambda to handle email communications via Amazon SES. Core Business Logic: Traffic is directed through an Application Load Balancer (ALB) located in the Public Subnet, then forwarded to Amazon ECS Fargate instances secured within a Private Subnet. Data \u0026amp; Security:\nRelational data is persistently stored in Amazon RDS within the Private Subnet. To optimize security and performance, the architecture utilizes VPC Endpoints to keep traffic strictly within the AWS internal network:\nStatic assets and images stored in Amazon S3 are accessed securely via S3 Endpoints. Container images are pulled directly from Amazon ECR via ECR Endpoints. By leveraging these endpoints, the system eliminates the need for a NAT Gateway, thereby reducing costs and minimizing public internet exposure.\nDevOps \u0026amp; Monitoring:\nGitLab is used for source code management and CI/CD, automatically pushing deployments to Amplify (Frontend) and container images to ECR (Backend). AWS Services Used Route 53: DNS service, responsible for routing the domain (Edge layer) to the application. AWS Amplify Hosting: Hosts the web interface (frontend) and can integrate with CDN/WAF. In the diagram, it receives traffic from Route 53. Amazon API Gateway: The main entry point (Gateway), receiving and routing all requests from the frontend/Amplify to processing services. AWS Lambda (Bedrock): Handles AI/Generative AI tasks (prediction/content generation) by communicating with Amazon Bedrock. AWS Lambda (SES): Handles asynchronous tasks, such as processing notifications to send emails via AWS SES. Amazon Bedrock: General AI service (Gen AI), providing foundation models to execute intelligent business operations. AWS SES: Email sending service, performs the sending of notifications, quotes, or processing results from Lambda. VPC: Virtual network environment containing and protecting AWS resources (like ALB, ECS Fargate, RDS). ALB (Application Load Balancer): Load balancer, distributing traffic from API Gateway to application containers running on ECS Fargate. Amazon ECS Fargate: Runs the backend application as containers without server management, handling core business logic. Amazon RDS: Provides a relational database, placed in a Private Subnet to store structured data. Amazon S3: Stores multimedia files like photos or other large data. ECR: Repository for application container images (Docker), used by ECS Fargate for deployment. AWS CloudWatch: Monitoring service, collecting logs and metrics from the entire system to track performance and detect issues. Component Design Request Handling: Amazon Route 53 routes user domain requests to AWS Amplify Hosting, where the frontend interface is hosted. From there, API requests are forwarded to Amazon API Gateway, which acts as the central entry point to receive and route all incoming traffic.\nBusiness Logic Processing:\nCore Logic: All primary business operations are handled by containerized applications running on Amazon ECS Fargate, deployed within a Private Subnet to ensure maximum security. AI \u0026amp; Asynchronous Tasks: Generative AI tasks are processed by AWS Lambda interacting with Amazon Bedrock. Auxiliary tasks, such as email notifications, are handled by separate AWS Lambda functions triggering Amazon SES. Network Infrastructure:\nPublic Subnet: Hosts the Application Load Balancer (ALB) to receive and distribute external traffic. Private Subnet: Dedicated to sensitive resources including ECS Fargate and Amazon RDS, ensuring they are isolated from direct public internet access. VPC Endpoints: The system explicitly utilizes S3 Endpoints and ECR Endpoints. This design allows ECS Fargate to pull container images and access file storage securely within the AWS internal network, without traversing the public internet. Data Storage:\nAmazon RDS: Stores sensitive, structured relational data. Amazon S3: Stores multimedia files and large datasets. Deployment and Monitoring: The deployment pipeline is managed via GitLab, which triggers updates to AWS Amplify (Frontend) and pushes Docker images to Amazon ECR (Backend). Amazon CloudWatch provides comprehensive monitoring of performance logs and metrics across all services, from ECS and Lambda to RDS.\n4. Technical Implementation Implementation Phases The development project for the RenGen Smart Electric Vehicle Maintenance Platform ‚Äî including the integration of an AI virtual assistant and a service management system ‚Äî undergoes 4 phases:\nResearch and Architectural Design: Research suitable technologies (React.js, Spring Boot, AWS Bedrock) and design a system architecture combining Containers (ECS) and Serverless (Lambda) on AWS (1 month prior to commencement). Cost Estimation and Feasibility Check: Use the AWS Pricing Calculator to estimate operating costs for core services such as ECS Fargate, RDS, and token costs for Amazon Bedrock, and propose the most feasible solution. Architecture Adjustment for Cost/Solution Optimization: Refine the architecture, select appropriate configurations for ECS Fargate and RDS, and optimize Lambda runtime (timeouts) to balance AI processing performance and cost. Development, Testing, and Deployment: Program the React.js application (Frontend) and Spring Boot (Backend), integrate the Bedrock Agent, deploy CI/CD pipelines via GitLab, package Docker images to ECR, and launch operations on ECS. Technical Requirements Technical Requirements User Interface (Frontend): Practical knowledge of React.js to build scheduling interfaces and chat with the AI virtual assistant. Use AWS Amplify to automate the deployment process (Hosting), connect with Amazon API Gateway to send secure processing requests, ensuring a smooth user experience on all devices. Core System (Backend \u0026amp; Infrastructure): In-depth knowledge of Java/Spring Boot to develop maintenance business logic. The application is packaged using Docker, with images stored on AWS ECR and running on Amazon ECS Fargate. Requires understanding of Amazon RDS for relational databases (storing vehicle profiles, maintenance history). Specifically, requires AWS Lambda (Python) programming skills to connect with Amazon Bedrock (AI/Chatbot processing) and AWS SES (sending asynchronous email notifications). Manage detailed user authentication and authorization (customers/technicians) via Amazon Cognito. 5. Timeline \u0026amp; Milestones Project Timeline\nPhase 1 (Week 1-2): Design and Foundation:: Analyze \u0026amp; Design detailed AWS architecture (VPC, Subnets, Security Groups). Design Database (RDS Schema) and define APIs (Swagger/OpenAPI). Configure infrastructure environment: Setup VPC (Public/Private Subnets), IAM Roles, and Amazon Cognito (User Pools). Setup CI/CD: Configure Pipeline on GitLab to automatically build Docker Images, push to Amazon ECR, and deploy Frontend to AWS Amplify. Phase 2 (Week 3-4): Core Service Flow Development:: Develop Customer flow (Frontend/Backend): Registration/Login, Vehicle Profile Management, Appointment Scheduling (stored in RDS). Develop Service Advisor flow: Vehicle Reception, Create Quotations and Repair Orders. Ph√°t tri·ªÉn lu·ªìng K·ªπ thu·∫≠t vi√™n: Xem danh s√°ch vi·ªác c·∫ßn l√†m (Task list), C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô b·∫£o d∆∞·ª°ng v√† t·∫£i ·∫£nh/video l√™n Amazon S3. Phase 3 (Week 5-6): Administration \u0026amp; Advanced Feature Development:: Build Administration Module: Report Dashboard, Spare Parts Management (Inventory), and Personnel Management.Build Administration Module: Report Dashboard, Spare Parts Management (Inventory), and Personnel Management. Write AWS Lambda to connect Amazon Bedrock Agent (AI Chatbot for customer support) and expose via API Gateway. Write AWS Lambda to trigger AWS SES for sending automatic notification emails/quotations to customers. Configure NAT Gateway so resources in Private Subnet (Lambda, ECS) can securely connect to the Internet/AWS Services. Phase 4 (Week 7-8): Testing, Optimization, and Operation:: Internal User Acceptance Testing (UAT) to ensure the flow from Web -\u0026gt; API Gateway -\u0026gt; Lambda/ECS -\u0026gt; DB operates smoothly. Optimize security: Configure AWS WAF (block SQL Injection, XSS) and review IAM access rights. Operational monitoring: Setup Dashboard on Amazon CloudWatch to track logs and metrics of ECS Fargate and Lambda. Official deployment. 6. Budget Estimation Infrastructure Costs\nAmazon ECS Fargate: ~11.00 USD/month. Application Load Balancer (ALB): ~16.43 USD/month. Amazon Bedrock (AI): ~5.00 USD/month (Calculated by Token count). AWS Lambda: 0.00 USD/month (Free Tier). Amazon RDS \u0026amp; ElastiCache: 0.00 USD/month (Free Tier). S3 Standard: ~0.15 USD/month. AWS Amplify \u0026amp; API Gateway: ~0.50 USD/month. Amazon CloudWatch: 0.00 USD/month (Free Tier). Amazon SES: 0.00 USD/month (Free Tier). Total: ~32.63/month.\n7. Risk Assessment Risk Matrix System downtime: High impact, low probability. Security breach/Data loss: Very high impact, low probability. Operational cost overrun: Medium impact, medium probability. Mitigation Strategies System: Deploy infrastructure across Multi-AZ for RDS and ECS Fargate. Use Application Load Balancer for automatic load distribution and recovery. Security: Use AWS WAF to filter malicious requests. Strict authorization with Amazon Cognito and apply least privilege principle. Backend placed in a separate network (Private Subnet). Cost: Use AWS Budgets to set alerts when costs exceed thresholds. Regularly monitor and optimize resources (right-sizing) to avoid waste. Incorrect AI response: Medium impact, medium probability. Contingency Plans System: Deploy ECS Fargate and RDS infrastructure across Multi-AZ to ensure high availability. Use Application Load Balancer for automatic load coordination and Auto-scaling to expand Tasks when traffic spikes. AI Quality: Limit Bedrock Agent response scope via strict Prompt Engineering (System Prompts) and only allow information retrieval from moderated Knowledge Bases. Enable Automated Backups for RDS and Point-in-time Recovery to restore data to any point in time. 8. Expected Outcomes Technical Improvements: Technical Improvements: Successfully build a modern Hybrid Architecture combining Microservices (ECS Fargate) and Serverless (Lambda, Bedrock), ensuring flexible scalability without managing physical servers.\nLong-term Value Enhance customer experience: AI virtual assistant operating 24/7 helps reduce waiting time, increasing appointment conversion rates and car owner satisfaction.\nData assets: Maintenance history and interaction behavior data are centrally stored on RDS/S3, creating a premise for deploying AI Predictive Maintenance models for electric vehicle batteries and motors in the future.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Master advanced NLP techniques: BERT architecture, Fine-tuning, and Multi-Task Training strategies. Finalize the product interface (UI) and conduct comprehensive system testing. Optimize system architecture by shifting AI integration to a Serverless model. Implement direct Frontend-to-Bedrock integration using AWS Lambda. Tasks executed this week: Day Task Start Date End Date Resources Mon - Studied the BERT architecture and fine-tuning techniques - Reviewed multi-task training strategies - Completed a fine-tuning lab using available datasets 10/11/2025 10/11/2025 https://www.coursera.org/learn/attention-models-in-nlp/ Tue - Finalized the user interface and verified core features - Conducted product-level testing 11/11/2025 11/11/2025 Wed - Held feature review meeting and prioritized enhancements - Performed system optimization tasks 12/11/2025 12/11/2025 Thu - Continued system optimization and resolved integration issues affecting the Bedrock-powered chatbot 13/11/2025 13/11/2025 https://cloudjourney.awsstudygroup.com/ Fri - Implemented an AWS Lambda function to enable Frontend-to-Bedrock interaction - Investigated front-end integration patterns for Bedrock 14/11/2025 14/11/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Advanced NLP (BERT):\nReviewed BERT internals and completed fine-tuning experiments. Applied multi-task training principles where appropriate. Product completion:\nFinalized the UI and validated core features through testing. Architectural improvements:\nMigrated the chatbot integration toward a serverless model. Implemented Lambda-based Frontend-to-Bedrock calls to reduce latency and improve scalability. Tuned system performance according to test results and feedback. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Complete and formalize all project documentation (Internship Report, Proposal, and Workshop materials). Synchronize the technical architecture diagram with the actual implemented solution. Finalize the integration of all AWS services. Execute the final deployment of the project to the AWS Cloud and perform post-deployment testing. Tasks completed this week: Day Task Start Date End Date Resources Mon - Continued drafting the Internship Report and performed targeted system optimizations 17/11/2025 17/11/2025 Tue - Updated the architecture diagram to reflect recent changes - Revised the Project Proposal and Workshop materials - Convened a team meeting to align on completion tasks 18/11/2025 18/11/2025 Wed - Finalized project deliverables and completed remaining edits 19/11/2025 19/11/2025 Thu - Performed final AWS service integration and end-to-end validation 20/11/2025 20/11/2025 https://cloudjourney.awsstudygroup.com/ Fri - Conducted post-deployment verification and fixed any remaining issues 21/11/2025 21/11/2025 Week 11 Achievements: Documentation:\nCompleted and polished the Internship Report. Updated the Project Proposal and Workshop materials to match the final implementation. Architecture alignment:\nSynchronized the architecture diagram with the deployed solution. Obtained team sign-off on the final design. Deployment and validation:\nIntegrated core AWS services (Bedrock, Lambda, databases) into the system. Deployed the final application to AWS and performed thorough post-deployment testing and remediation. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Complete and polish the Internship Report. Identify, debug, and resolve all remaining issues found during the deployment phase. Implement final system optimizations based on team consensus. Conduct a final internal review and user experience session for the semester project. Tasks executed this week: Day Task Start Date End Date Resources Mon - Continued writing the Internship Report and addressed deployment-related defects 24/11/2025 24/11/2025 Tue - Performed targeted optimizations and convened a team session to triage and resolve identified issues 25/11/2025 25/11/2025 Wed - Implemented the agreed technical fixes and validated the system accordingly 26/11/2025 26/11/2025 Thu - Continued validation and hardening of the system following group recommendations 27/11/2025 27/11/2025 Fri - Finalized the project and conducted an internal demo and user experience session with the team 28/11/2025 28/11/2025 Week 12 Achievements: Documentation and reporting:\nFinalized and edited the Internship Report for submission. Quality assurance \u0026amp; remediation:\nIdentified and prioritized deployment defects. Implemented corrective actions in collaboration with the team. System stabilization:\nApplied agreed fixes and validated system stability and performance. Final delivery:\nConducted the end-of-semester demo and user experience session; all core features validated for submission. "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - How BeyondTrust embedded Amazon QuickSight for identity security insights BeyondTrust leveraged Amazon QuickSight to build faster, more scalable, and cost-efficient identity security analytics. QuickSight enabled rapid dashboard development, seamless embedding, multi-tenant security, and a streamlined CI/CD pipeline that reduced deployment time from weeks to days and cut operational costs by 60%. With a custom UI and automated workflows, BeyondTrust improved reporting efficiency and customer value. They plan to expand QuickSight with Amazon Q, enhanced editor features, pixel-perfect reports, and multi-region deployments to further strengthen their analytics capabilities.\nBlog 2 - How to securely deliver business intelligence to internal-facing applications with Amazon QuickSight This article explains how to securely integrate Amazon QuickSight into internal applications by leveraging a company‚Äôs centralized access management via IAM Identity Center. It covers three access models: direct dashboard access, embedding in an internal website, and embedding in a restricted SPA. The most advanced approach uses user JWTs along with API Gateway and Lambda to generate dynamic QuickSight embed URLs, enabling seamless, pop-up-free embedded dashboards while enforcing proper authorization. This architecture allows organizations to safely deliver rich business intelligence experiences within existing identity and access control frameworks.\nBlog 3 - Software developer career paths: 2025 job guide This article provides a comprehensive guide to 17 software development career paths for 2025, ranging from foundational roles (Front-end, Back-end, Full-stack) to specialized fields (AI/ML, Cloud, DevOps, Security). Each path details average salaries, required technical skills, and portfolio ideas, ultimately emphasizing that career success is like a \u0026ldquo;menu\u0026rdquo;‚Äîa personal choice based on your individual goals and preferences.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.3-vpc/",
	"title": "VPC",
	"tags": [],
	"description": "",
	"content": "Intro VPC (Virtual Private Cloud) is a logically isolated virtual private network space in AWS Cloud. It acts as your personal data center in the cloud, giving you complete control over the network environment.\nCore components: Route Table Subnets Internet Gateway Security Groups Create VPC Open the Amazon VPC console Choose Your VPCs, then click Create VPC In the create VPC console: Specify name of the Name tag: my-vpc-01 IPv4 CIDR : 10.0.0.0/16 Then click Create VPC Content Create Route Table \u0026amp; Internet Gateway Create Subnets Create Security Groups "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "http://localhost:1313/workshop-template/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event‚Äôs content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Deploy ReGenZet Management System To AWS Overview ReGenZet is an enterprise-grade EV garage management platform. The objective of this workshop is to design and deploy a secure, cost-optimized, and highly automated cloud infrastructure on AWS to host ApexEV\u0026rsquo;s frontend, backend, storage, and serverless AI/ML functions.\nKey architectural principles:\nSecurity-first: least-privilege IAM, encrypted data at rest and in transit, network isolation and controlled service endpoints. Cost optimization: use managed services with pay-as-you-go models, right-sizing, and automated lifecycle policies for storage and compute. Automation \u0026amp; Observability: Infrastructure-as-Code, CI/CD pipelines, centralized logging, and automated monitoring/alarms. Core services used in this workshop:\nAWS ECS (Fargate) ‚Äî run backend microservices without managing servers. AWS Amplify ‚Äî host the frontend, provide CI/CD for web clients and manage hosting. Amazon RDS ‚Äî managed relational database for transactional data. Amazon S3 ‚Äî object storage for media, backups, and static assets. AWS Lambda ‚Äî serverless functions for AI/ML processing pipelines, notifications and background tasks. This workshop contains hands-on modules covering the end-to-end stack and best practices for each layer.\nContent Workshop overview Project Architecture VPC of project Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "http://localhost:1313/workshop-template/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at [First Cloud Journey / AWS Study Group] from September 15, 2025 to November 28, 2025, I had the opportunity to bridge the gap between academic theory and professional application in the field of Cloud Computing and Artificial Intelligence.\nI actively participated in the \u0026ldquo;Development of an Intelligent AI Chatbot using AWS Bedrock and Serverless Architecture\u0026rdquo; project. Through this intensive period, I significantly improved my skills in Cloud Infrastructure (AWS VPC, EC2, Lambda), Natural Language Processing (BERT, Transformers), and Full-stack System Integration.\nIn terms of work ethic, I consistently strived to master new technologies (such as GenAI and Serverless), complied with project timelines, and collaborated effectively with mentors and team members to optimize the system architecture.\nTo objectively reflect on my internship period, I evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Grasping AWS core services, NLP algorithms, and applying them to build a functional product. ‚úÖ ‚òê ‚òê 2 Ability to learn Rapidly adapting to new tech stacks (AWS Bedrock, Hugging Face) within a short timeframe. ‚úÖ ‚òê ‚òê 3 Proactiveness Proposing architectural changes (Migration to Serverless) to improve performance. ‚úÖ ‚òê ‚òê 4 Sense of responsibility Committing to deadlines for the Proposal, Midterm Exam, and Final Deployment. ‚úÖ ‚òê ‚òê 5 Discipline Adhering to strict schedules, reporting guidelines, and organizational rules. ‚òê ‚úÖ ‚òê 6 Progressive mindset Receptive to feedback regarding Architecture Diagrams and UI/UX design. ‚úÖ ‚òê ‚òê 7 Communication Articulating technical concepts clearly in weekly reports and team meetings. ‚òê ‚úÖ ‚òê 8 Teamwork Coordinating with Front-end and Back-end members to resolve integration conflicts. ‚úÖ ‚òê ‚òê 9 Professional conduct Maintaining a respectful and professional attitude towards mentors and peers. ‚úÖ ‚òê ‚òê 10 Problem-solving skills Debugging integration errors and optimizing system latency during the testing phase. ‚òê ‚úÖ ‚òê 11 Contribution to project/team Delivering a working AI module and finalizing the system deployment on AWS. ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period. ‚úÖ ‚òê ‚òê Areas for Improvement Enhance Operational Discipline: I need to strengthen my adherence to strict organizational workflows and time management to avoid last-minute crunches during deployment phases. Deepen Problem-Solving Strategy: Move from \u0026ldquo;reactive debugging\u0026rdquo; to \u0026ldquo;proactive architectural design\u0026rdquo; to anticipate and prevent system errors before they occur. "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "http://localhost:1313/workshop-template/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "http://localhost:1313/workshop-template/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/workshop-template/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]