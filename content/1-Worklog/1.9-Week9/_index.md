---
title: "Week 9 Worklog"
date: "2025-11-05"
weight: 1
chapter: false
pre: " <b> 1.9. </b> "
---


### Week 9 Objectives:

* Deep dive into modern NLP architectures: Transformers, Attention Mechanisms, and Decoders.
* Explore Transfer Learning and Large Language Models (LLMs) like BERT, GPT, and T5.
* Implement Generative AI solutions using AWS Bedrock.
* Optimize the Final Project architecture and refine the Chatbot's performance/UI.

### Tasks performed this week:
| Day | Task                                                                                                                                                                                                                                | Start Date | End Date   | Resources                                                  |
| --- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------- | ---------- | ---------------------------------------------------------- |
| Mon | - Studied differences between Transformers and RNNs <br> - Reviewed scaled dot-product attention and masked self-attention <br> - Analyzed transformer decoders                                                              | 03/11/2025 | 03/11/2025 | <https://www.coursera.org/learn/attention-models-in-nlp/>  |
| Tue | - Completed a Transformer summarization lab <br> - Optimized the chatbot logic and refined the user interface                                                                                                                                        | 04/11/2025 | 04/11/2025 | <https://www.coursera.org/learn/attention-models-in-nlp/>  |
| Wed | - Held a meeting to optimize the AWS service selection <br> - Studied transfer learning techniques and surveyed major LLMs (ELMo, GPT, BERT, T5)                                                                       | 05/11/2025 | 05/11/2025 | <https://www.coursera.org/learn/attention-models-in-nlp/>  |
| Thu | - Explored AWS Bedrock and experimented with integrating it into the AI chatbot                                                                                                                                                       | 06/11/2025 | 06/11/2025 |                                                            |
| Fri | - Prepared and curated training/response data for the Bedrock-backed chatbot <br> - Performed testing and resolved data-processing issues                                                                                                                   | 07/11/2025 | 07/11/2025 |                                                            |


### Week 9 Achievements: 



* Advanced NLP architectures:
  - Distinguished Transformers from RNN architectures and examined attention variants.
  - Implemented a Transformer-based summarizer in practical labs.

* Large language model exposure:
  - Studied transfer learning paradigms and surveyed prominent LLMs (ELMo, GPT, BERT, T5).

* Generative AI on AWS:
  - Integrated AWS Bedrock features to enhance the chatbot's generative capabilities.
  - Curated and preprocessed context data and debugged integration issues.

* Product refinement:
  - Improved chatbot UI and optimized the chosen AWS service stack for the final deployment.